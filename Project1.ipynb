{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Identification using Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Image Preprocessing\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import csv\n",
    "from skimage import exposure\n",
    "\n",
    "#A tua mina, tem ganda vagina\n",
    "#Mas n√£o foste tu quem escavou bro, foi o Chico da Tina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function\n",
    "Resizes and Normalizes all pictures in all of our datasets and then stores them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(file_name):\n",
    "    path = './Dataset/'   # Change this for the path corresponding to your base Dataset directory\n",
    "\n",
    "    img_infos = {'labels':[],'images':[]}\n",
    "\n",
    "    with open(path+file_name,\"r\") as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',', quotechar='|')\n",
    "        next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "\n",
    "            img = cv2.imread(path+row[-1], cv2.IMREAD_UNCHANGED)    \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #Convert image to Grayscale\n",
    "            \n",
    "            img = cv2.resize(img, (32,32),interpolation = cv2.INTER_AREA) #Resize the image to 32x32\n",
    "\n",
    "            #img = cv2.addWeighted( img, 1.2, img, 0, 0) #Increase contrast\n",
    "            \n",
    "            cv2.normalize(img, img, 0, 255, cv2.NORM_MINMAX) #Normalize\n",
    "            \n",
    "            img_eq = exposure.equalize_adapthist(img, clip_limit=0.03)\n",
    "            \n",
    "            img_infos['images'].append(img_eq) #Save Image pixels\n",
    "            img_infos['labels'].append(row[-2]) #Save image Label\n",
    "            \n",
    "    return img_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save treated data as Pickle\n",
    "Srsly, funniest thing I've ever seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished writing tests\n",
      "finished writing train\n"
     ]
    }
   ],
   "source": [
    "img_infos = preprocess_images(\"Test.csv\")\n",
    "with open(\"./Dataset/PickledData/Test.p\", 'wb') as pickle_rick:\n",
    "    pickle.dump(img_infos,pickle_rick)\n",
    "    \n",
    "print(\"finished writing tests\")\n",
    "img_infos = preprocess_images(\"Train.csv\")\n",
    "with open(\"./Dataset/PickledData/Train.p\", 'wb') as pickle_rick:\n",
    "    pickle.dump(img_infos,pickle_rick)\n",
    "print(\"finished writing train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "### Step 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Get the data into the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Dataset/PickledData/Test.p\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "with open(\"./Dataset/PickledData/Train.p\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "# Get the training set\n",
    "x_train, y_train = train['images'], train['labels']\n",
    "\n",
    "# Get the cross validation set\n",
    "x_cv = []\n",
    "y_cv = []\n",
    "for i in range(int(0.2*len(x_train))): # Randomly move 20% of the training examples into the cross-validation set\n",
    "    index = random.randint(0, len(x_train)-1)\n",
    "    x_cv.append(x_train.pop(index))\n",
    "    y_cv.append(y_train.pop(index))\n",
    "    \n",
    "# Get the test set\n",
    "x_test, y_test = test['images'], test['labels']\n",
    "\n",
    "# Convert all our sets into numPy Arrays\n",
    "x_train = np.array(x_train).reshape((len(x_train), 32, 32, 1))\n",
    "y_train = keras.utils.to_categorical(np.array(y_train))\n",
    "\n",
    "x_cv = np.array(x_cv).reshape((len(x_cv), 32, 32, 1))\n",
    "y_cv = keras.utils.to_categorical(np.array(y_cv))\n",
    "\n",
    "x_test = np.array(x_test).reshape((len(x_test), 32, 32, 1))\n",
    "y_test = keras.utils.to_categorical(np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Treat the data (Normalize and Shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training set\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 200\n",
    "LOSS = keras.losses.categorical_crossentropy\n",
    "OPT = keras.optimizers.Adam\n",
    "LEARNING_RATE = 0.002 # Define the learning rate to be used by our optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNetModel(no_output=6, rgb=True, dropout=0):\n",
    "    input_shape = (32,32,3) if rgb else (32,32,1) # Define the image's size\n",
    "    \n",
    "    model = keras.Sequential() # Define the Keras Model\n",
    "    \n",
    "    # First Layer - Convolutional ; Input = 32x32x3 for RGB , 32x32x1 for GrayScale ; Output = 28x28x6\n",
    "    model.add(keras.layers.Conv2D(\n",
    "                filters=no_output, kernel_size=(3, 3),\n",
    "                activation='relu', input_shape=input_shape\n",
    "                )\n",
    "             )\n",
    "    \n",
    "    # Second Layer - Subsampling (Average Pooling) ; Input = 28x28x6 ; Output = 14x14x6\n",
    "    model.add(keras.layers.AveragePooling2D())\n",
    "    \n",
    "    # Third Layer - Convolutional ; Input = 14x14x6 ; Output = 10x10x16\n",
    "    model.add(keras.layers.Conv2D(\n",
    "                    filters=16, kernel_size=(3, 3),\n",
    "                    activation='relu'\n",
    "                    )\n",
    "                 )\n",
    "    \n",
    "    # Fourth Layer - Subsampling (Average Pooling) ; Input = 10x10x16 ; Output = 5x5x16\n",
    "    model.add(keras.layers.AveragePooling2D())\n",
    "    \n",
    "    model.add(keras.layers.Flatten()) # Flatten the last layer's output to pass it to the Fully Connected layers\n",
    "    \n",
    "    # Fifth Layer - Fully Connected ; Input = 5x5x16 ; Output = 120x1\n",
    "    model.add(keras.layers.Dense(units=120, activation='relu'))\n",
    "\n",
    "    # Sixth Layer - Fully Connected ; Input = 120x1 ; Output = 84x1\n",
    "    model.add(keras.layers.Dense(units=84, activation='relu'))\n",
    "    \n",
    "    model.add(keras.layers.Dropout(dropout))\n",
    "    \n",
    "    # Output Layer - Output = 43x1\n",
    "    model.add(keras.layers.Dense(units=43, activation = 'softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Create the model and define evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNetModel(rgb=False, dropout=0.50)\n",
    "\n",
    "model.compile(loss=LOSS,\n",
    "              optimizer=OPT(learning_rate=LEARNING_RATE),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Insert the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31368 samples, validate on 7841 samples\n",
      "Epoch 1/50\n",
      "31368/31368 [==============================] - 3s 105us/sample - loss: 2.3795 - accuracy: 0.3457 - val_loss: 0.9822 - val_accuracy: 0.7252\n",
      "Epoch 2/50\n",
      "31368/31368 [==============================] - 3s 91us/sample - loss: 0.9658 - accuracy: 0.7001 - val_loss: 0.4955 - val_accuracy: 0.8545\n",
      "Epoch 3/50\n",
      "31368/31368 [==============================] - 3s 93us/sample - loss: 0.6254 - accuracy: 0.8052 - val_loss: 0.3438 - val_accuracy: 0.9005\n",
      "Epoch 4/50\n",
      "31368/31368 [==============================] - 3s 98us/sample - loss: 0.4751 - accuracy: 0.8505 - val_loss: 0.2518 - val_accuracy: 0.9241\n",
      "Epoch 5/50\n",
      "31368/31368 [==============================] - 3s 96us/sample - loss: 0.3933 - accuracy: 0.8758 - val_loss: 0.2231 - val_accuracy: 0.9351\n",
      "Epoch 6/50\n",
      "31368/31368 [==============================] - 3s 93us/sample - loss: 0.3273 - accuracy: 0.8962 - val_loss: 0.1690 - val_accuracy: 0.9523\n",
      "Epoch 7/50\n",
      "31368/31368 [==============================] - 3s 96us/sample - loss: 0.2754 - accuracy: 0.9155 - val_loss: 0.1457 - val_accuracy: 0.9554\n",
      "Epoch 8/50\n",
      "31368/31368 [==============================] - 3s 95us/sample - loss: 0.2427 - accuracy: 0.9217 - val_loss: 0.1282 - val_accuracy: 0.9625\n",
      "Epoch 9/50\n",
      "31368/31368 [==============================] - 3s 102us/sample - loss: 0.2057 - accuracy: 0.9343 - val_loss: 0.1242 - val_accuracy: 0.9619\n",
      "Epoch 10/50\n",
      "31368/31368 [==============================] - 4s 113us/sample - loss: 0.1796 - accuracy: 0.9420 - val_loss: 0.1007 - val_accuracy: 0.9707\n",
      "Epoch 11/50\n",
      "31368/31368 [==============================] - 3s 109us/sample - loss: 0.1617 - accuracy: 0.9481 - val_loss: 0.0923 - val_accuracy: 0.9721\n",
      "Epoch 12/50\n",
      "31368/31368 [==============================] - 4s 137us/sample - loss: 0.1506 - accuracy: 0.9520 - val_loss: 0.0875 - val_accuracy: 0.9744\n",
      "Epoch 13/50\n",
      "31368/31368 [==============================] - 4s 127us/sample - loss: 0.1317 - accuracy: 0.9581 - val_loss: 0.0869 - val_accuracy: 0.9746\n",
      "Epoch 14/50\n",
      "31368/31368 [==============================] - 4s 128us/sample - loss: 0.1251 - accuracy: 0.9588 - val_loss: 0.0815 - val_accuracy: 0.9754\n",
      "Epoch 15/50\n",
      "31368/31368 [==============================] - 4s 131us/sample - loss: 0.1092 - accuracy: 0.9650 - val_loss: 0.1204 - val_accuracy: 0.9619\n",
      "Epoch 16/50\n",
      "31368/31368 [==============================] - 4s 126us/sample - loss: 0.1049 - accuracy: 0.9647 - val_loss: 0.0772 - val_accuracy: 0.9772\n",
      "Epoch 17/50\n",
      "31368/31368 [==============================] - 4s 127us/sample - loss: 0.0935 - accuracy: 0.9702 - val_loss: 0.0736 - val_accuracy: 0.9783\n",
      "Epoch 18/50\n",
      "31368/31368 [==============================] - 4s 121us/sample - loss: 0.0858 - accuracy: 0.9721 - val_loss: 0.0655 - val_accuracy: 0.9798\n",
      "Epoch 19/50\n",
      "31368/31368 [==============================] - 4s 127us/sample - loss: 0.0767 - accuracy: 0.9741 - val_loss: 0.0701 - val_accuracy: 0.9802\n",
      "Epoch 20/50\n",
      "31368/31368 [==============================] - 4s 127us/sample - loss: 0.0735 - accuracy: 0.9765 - val_loss: 0.0653 - val_accuracy: 0.9802\n",
      "Epoch 21/50\n",
      "31368/31368 [==============================] - 4s 125us/sample - loss: 0.0654 - accuracy: 0.9775 - val_loss: 0.0591 - val_accuracy: 0.9834\n",
      "Epoch 22/50\n",
      "31368/31368 [==============================] - 4s 127us/sample - loss: 0.0625 - accuracy: 0.9791 - val_loss: 0.0658 - val_accuracy: 0.9820\n",
      "Epoch 23/50\n",
      "31368/31368 [==============================] - 4s 124us/sample - loss: 0.0622 - accuracy: 0.9786 - val_loss: 0.0666 - val_accuracy: 0.9819\n",
      "Epoch 24/50\n",
      "31368/31368 [==============================] - 4s 135us/sample - loss: 0.0556 - accuracy: 0.9819 - val_loss: 0.0697 - val_accuracy: 0.9816\n",
      "Epoch 25/50\n",
      "31368/31368 [==============================] - 4s 130us/sample - loss: 0.0505 - accuracy: 0.9828 - val_loss: 0.0612 - val_accuracy: 0.9838\n",
      "Epoch 26/50\n",
      "31368/31368 [==============================] - 4s 123us/sample - loss: 0.0504 - accuracy: 0.9828 - val_loss: 0.0655 - val_accuracy: 0.9828\n",
      "Epoch 27/50\n",
      "31368/31368 [==============================] - 4s 133us/sample - loss: 0.0455 - accuracy: 0.9852 - val_loss: 0.0589 - val_accuracy: 0.9844\n",
      "Epoch 28/50\n",
      "31368/31368 [==============================] - 4s 123us/sample - loss: 0.0414 - accuracy: 0.9863 - val_loss: 0.0658 - val_accuracy: 0.9829\n",
      "Epoch 29/50\n",
      "31368/31368 [==============================] - 4s 127us/sample - loss: 0.0426 - accuracy: 0.9856 - val_loss: 0.0599 - val_accuracy: 0.9846\n",
      "Epoch 30/50\n",
      "31368/31368 [==============================] - 4s 131us/sample - loss: 0.0378 - accuracy: 0.9869 - val_loss: 0.0627 - val_accuracy: 0.9834\n",
      "Epoch 31/50\n",
      "31368/31368 [==============================] - 4s 131us/sample - loss: 0.0412 - accuracy: 0.9853 - val_loss: 0.0603 - val_accuracy: 0.9851\n",
      "Epoch 32/50\n",
      "31368/31368 [==============================] - 4s 129us/sample - loss: 0.0363 - accuracy: 0.9880 - val_loss: 0.0688 - val_accuracy: 0.9841\n",
      "Epoch 33/50\n",
      "31368/31368 [==============================] - 4s 119us/sample - loss: 0.0353 - accuracy: 0.9884 - val_loss: 0.0684 - val_accuracy: 0.9844\n",
      "Epoch 34/50\n",
      "31368/31368 [==============================] - 4s 127us/sample - loss: 0.0358 - accuracy: 0.9879 - val_loss: 0.0553 - val_accuracy: 0.9879\n",
      "Epoch 35/50\n",
      "31368/31368 [==============================] - 4s 126us/sample - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.0663 - val_accuracy: 0.9819\n",
      "Epoch 36/50\n",
      "31368/31368 [==============================] - 4s 123us/sample - loss: 0.0298 - accuracy: 0.9899 - val_loss: 0.0655 - val_accuracy: 0.9844\n",
      "Epoch 37/50\n",
      "31368/31368 [==============================] - 4s 132us/sample - loss: 0.0271 - accuracy: 0.9903 - val_loss: 0.0550 - val_accuracy: 0.9880\n",
      "Epoch 38/50\n",
      "31368/31368 [==============================] - 4s 128us/sample - loss: 0.0309 - accuracy: 0.9898 - val_loss: 0.0634 - val_accuracy: 0.9853\n",
      "Epoch 39/50\n",
      "31368/31368 [==============================] - 4s 136us/sample - loss: 0.0290 - accuracy: 0.9896 - val_loss: 0.0598 - val_accuracy: 0.9861\n",
      "Epoch 40/50\n",
      "31368/31368 [==============================] - 4s 136us/sample - loss: 0.0245 - accuracy: 0.9916 - val_loss: 0.0704 - val_accuracy: 0.9853\n",
      "Epoch 41/50\n",
      "31368/31368 [==============================] - 4s 129us/sample - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.0584 - val_accuracy: 0.9856\n",
      "Epoch 42/50\n",
      "31368/31368 [==============================] - 4s 136us/sample - loss: 0.0227 - accuracy: 0.9923 - val_loss: 0.0631 - val_accuracy: 0.9861\n",
      "Epoch 43/50\n",
      "31368/31368 [==============================] - 4s 129us/sample - loss: 0.0229 - accuracy: 0.9918 - val_loss: 0.0688 - val_accuracy: 0.9852\n",
      "Epoch 44/50\n",
      "31368/31368 [==============================] - 4s 135us/sample - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.0625 - val_accuracy: 0.9864\n",
      "Epoch 45/50\n",
      "31368/31368 [==============================] - 4s 126us/sample - loss: 0.0245 - accuracy: 0.9915 - val_loss: 0.0648 - val_accuracy: 0.9861\n",
      "Epoch 46/50\n",
      "31368/31368 [==============================] - 4s 122us/sample - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.0686 - val_accuracy: 0.9852\n",
      "Epoch 47/50\n",
      "31368/31368 [==============================] - 4s 127us/sample - loss: 0.0224 - accuracy: 0.9920 - val_loss: 0.0749 - val_accuracy: 0.9827\n",
      "Epoch 48/50\n",
      "31368/31368 [==============================] - 4s 121us/sample - loss: 0.0195 - accuracy: 0.9937 - val_loss: 0.0642 - val_accuracy: 0.9869\n",
      "Epoch 49/50\n",
      "31368/31368 [==============================] - 5s 147us/sample - loss: 0.0206 - accuracy: 0.9935 - val_loss: 0.0746 - val_accuracy: 0.9842\n",
      "Epoch 50/50\n",
      "31368/31368 [==============================] - 4s 125us/sample - loss: 0.0187 - accuracy: 0.9938 - val_loss: 0.0599 - val_accuracy: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f505c33c2d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(x_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12630/12630 [==============================] - 1s 91us/sample - loss: 0.3670 - accuracy: 0.9449\n",
      "Test loss: 0.36699507998754505\n",
      "Test accuracy: 0.9448931\n"
     ]
    }
   ],
   "source": [
    "y_predictions = model.predict_classes(x_test)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Absolute matrix\n",
    "conf_mat_abs = confusion_matrix(np.argmax(y_test, axis=1), y_predictions)\n",
    "\n",
    "#Relative matrix\n",
    "conf_mat_rel = conf_mat.astype('float') / conf_mat.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cm = pd.DataFrame(conf_mat, index = [i for i in range(43)],\n",
    "#                  columns = [i for i in range(43)])\n",
    "#plt.figure(figsize = (100,100))\n",
    "\n",
    "#sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Accuracy per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(1,44), [conf_mat_rel[i][i] for i in range(43)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB vs Grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Preprocessing Parameter changes (Contrast, lighting, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Before data augmentation we had an accuracy of 94.5% for our test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
